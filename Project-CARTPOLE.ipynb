{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nShoutout to Differnt youtube tutorials, various forums which helped during whole implementation purposes\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Topic Name: Reinforcement Learning in Cart-pole Game\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "This Python notebook is being developed in accordance to the proposed topic of\n",
    "Reinforcement Learning, and to demonstrate the algorithm along with it's working code\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Reason To Select This Project:\n",
    "\n",
    "    You might have came across the automatic landing spacecrafts of Famous \"SpaceX\",\n",
    "    Founded by Elon musk, which had idea of developing solution of reusing spacecraft\n",
    "    (which generally gets destroyed on manual manuever) by using Advance Machine Learning\n",
    "    Technologies and Computer vision Techniques, to help in slef landing of spacecraft.\n",
    "    \n",
    "    Although the level of details, parameters and problems were varied and are out of scope\n",
    "    for the current level of project, we have tried to replicate the situation by using some\n",
    "    pre-existing State of the art tools, which would give us a insight in form of big picture\n",
    "    of spectacular problem.\n",
    "    \n",
    "    Along with the fact that Reinforcement Learning (field of studying and developing interaction \n",
    "    of machine with environment along with certain parameters/policy/rules, helps in learning and \n",
    "    delivering results in maximum possible efficient and automated way) is currently being field of \n",
    "    interest for many aspiring or experienced Scientist, is being used in the project to implement \n",
    "    the solution for the problem \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Shoutout to Differnt youtube tutorials, various forums which helped during whole implementation purposes\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/apoorve/.local/lib/python3.6/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/apoorve/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNote : \\n    Below are warnings about the methods/functions in packages which might/will change/depreceates \\n    in upcoming official newer version\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This Cell contains Imports of various packages of python which we will be using during implementation:\n",
    "\n",
    "1) gym : for using predefined environment for deploying and rendering of Reinforcement learning Algorithms\n",
    "\n",
    "2) Tensorflow, tflearn : for implementation of Neural Networks and using them for training the machine\n",
    "\n",
    "3) matplotlib, numpy, seaborn, time : Various computation and data visualisation package.\n",
    "\"\"\"\n",
    "import gym\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "Note : \n",
    "    Below are warnings about the methods/functions in packages which might/will change/depreceates \n",
    "    in upcoming official newer version\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here now initialising the evironment i.e. Cart-pole Environment\n",
    "in \"env\" variable\n",
    "\"\"\"\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter of the environment:  [0.01620511 0.0142888  0.02357856 0.03831228]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThese 4 values represent the cart position, cart velocity, pole angle, and the velocity of the tip of the pole.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To Test whether our environment is being initiallised or not,\n",
    "we will try to check it, by running and testing out on it\n",
    "\"\"\"\n",
    "\n",
    "env_temp = gym.make(\"CartPole-v0\")\n",
    "obs = env_temp.reset()\n",
    "print(\"Parameter of the environment: \",obs)\n",
    "\n",
    "\"\"\"\n",
    "These 4 values represent the cart position, cart velocity, pole angle, and the velocity of the tip of the pole.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample Static Environment\n",
    "\"\"\"\n",
    "obs = env_temp.reset()\n",
    "env_temp.render() # will render the environment in humanly visible format\n",
    "time.sleep(7)     # will wait till 7 Seconds\n",
    "env_temp.close()  # will close the rendered environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when step(0) is supplied for action, the resultant change in parameters.\n",
      "\n",
      "Parameter Observed: [ 0.03050329 -0.2270382  -0.03631862  0.27727727] 1.0 False\n",
      "Parameter Observed: [ 0.02596253 -0.4216237  -0.03077307  0.55828771] 1.0 False\n",
      "Parameter Observed: [ 0.01753005 -0.61630047 -0.01960732  0.84111876] 1.0 False\n",
      "Parameter Observed: [ 0.00520404 -0.81114935 -0.00278494  1.12757171] 1.0 False\n",
      "Parameter Observed: [-0.01101894 -1.0062347   0.01976649  1.41937984] 1.0 False\n",
      "Parameter Observed: [-0.03114364 -1.20159563  0.04815409  1.71817495] 1.0 False\n",
      "Parameter Observed: [-0.05517555 -1.39723531  0.08251759  2.02544613] 1.0 False\n",
      "Parameter Observed: [-0.08312026 -1.59310806  0.12302651  2.34248817] 1.0 False\n",
      "Parameter Observed: [-0.11498242 -1.78910372  0.16987627  2.67033822] 1.0 False\n",
      "Parameter Observed: [-0.15076449 -1.98502948  0.22328304  3.0097003 ] 1.0 True\n",
      "count: 0\n",
      "Parameter Observed: [-0.19046508 -2.18058977  0.28347704  3.36085988] 0.0 True\n",
      "count: 1\n",
      "Parameter Observed: [-0.23407688 -2.37536574  0.35069424  3.7235937 ] 0.0 True\n",
      "count: 2\n",
      "Parameter Observed: [-0.28158419 -2.56879713  0.42516612  4.09708449] 0.0 True\n",
      "count: 3\n",
      "Parameter Observed: [-0.33296013 -2.76017059  0.50710781  4.47985458] 0.0 True\n",
      "count: 4\n",
      "Parameter Observed: [-0.38816355 -2.94861989  0.5967049   4.8697362 ] 0.0 True\n",
      "count: 5\n",
      "Parameter Observed: [-0.44713594 -3.1331442   0.69409962  5.2638961 ] 0.0 True\n",
      "count: 6\n",
      "Parameter Observed: [-0.50979883 -3.31265019  0.79937754  5.65892656] 0.0 True\n",
      "count: 7\n",
      "Parameter Observed: [-0.57605183 -3.48602154  0.91255607  6.05100127] 0.0 True\n",
      "count: 8\n",
      "Parameter Observed: [-0.64577226 -3.65221536  1.0335761   6.43607338] 0.0 True\n",
      "count: 9\n",
      "\n",
      "When step(0) is supplied for action, the resultant change in parameters.\n",
      "\n",
      "Parameter Observed: [ 0.01638224  0.16475619  0.02056938 -0.33099916] 1.0 False\n",
      "Parameter Observed: [ 0.01967737  0.35957939  0.01394939 -0.61712518] 1.0 False\n",
      "Parameter Observed: [ 0.02686896  0.55450372  0.00160689 -0.90538224] 1.0 False\n",
      "Parameter Observed: [ 0.03795903  0.74960388 -0.01650076 -1.19755967] 1.0 False\n",
      "Parameter Observed: [ 0.05295111  0.94493545 -0.04045195 -1.49536815] 1.0 False\n",
      "Parameter Observed: [ 0.07184982  1.14052528 -0.07035931 -1.80040251] 1.0 False\n",
      "Parameter Observed: [ 0.09466032  1.33635982 -0.10636736 -2.11409609] 1.0 False\n",
      "Parameter Observed: [ 0.12138752  1.53237117 -0.14864928 -2.43766449] 1.0 False\n",
      "Parameter Observed: [ 0.15203494  1.72842046 -0.19740257 -2.77203751] 1.0 False\n",
      "Parameter Observed: [ 0.18660335  1.92427895 -0.25284332 -3.11777986] 1.0 True\n",
      "count: 0\n",
      "Parameter Observed: [ 0.22508893  2.1196077  -0.31519892 -3.4750037 ] 0.0 True\n",
      "count: 1\n",
      "Parameter Observed: [ 0.26748108  2.31393782 -0.384699   -3.84327993] 0.0 True\n",
      "count: 2\n",
      "Parameter Observed: [ 0.31375984  2.50665454 -0.46156459 -4.22155929] 0.0 True\n",
      "count: 3\n",
      "Parameter Observed: [ 0.36389293  2.69698964 -0.54599578 -4.60811879] 0.0 True\n",
      "count: 4\n",
      "Parameter Observed: [ 0.41783272  2.88402809 -0.63815816 -5.00055156] 0.0 True\n",
      "count: 5\n",
      "Parameter Observed: [ 0.47551329  3.0667349  -0.73816919 -5.395816  ] 0.0 True\n",
      "count: 6\n",
      "Parameter Observed: [ 0.53684798  3.24400723 -0.84608551 -5.79035182] 0.0 True\n",
      "count: 7\n",
      "Parameter Observed: [ 0.60172813  3.41475398 -0.96189254 -6.18025391] 0.0 True\n",
      "count: 8\n",
      "Parameter Observed: [ 0.67002321  3.57800071 -1.08549762 -6.56147291] 0.0 True\n",
      "count: 9\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample Dynamic Environment\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Here we try to see what are the actions we can do in the environment and how it changes the environment.\n",
    "\n",
    "Here we use step(0) and step(1) to move the cartpole left and right, by applying -1 and +1 unit force respectively.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "##################################################################################################################\n",
    "print(\"when step(0) is supplied for action, the resultant change in parameters.\\n\")\n",
    "count = 0                      # Counter variable to store the change in the state of cartpole\n",
    "obs = env_temp.reset()         # Reseting the environment before rendering the environment\n",
    "while count <= 9:\n",
    "    for i in range(200):\n",
    "        time.sleep(1)\n",
    "        env_temp.render()\n",
    "        obs, r, d, _ = env_temp.step(0) \n",
    "        \"\"\"\n",
    "        Here the Parameters:\n",
    "        1) obs : These 4 values represent the cart position, cart velocity, pole angle, \n",
    "            and the velocity of the tip of the pole.\n",
    "        2) r : state, it's a positive number if it is above and negative if it falls.\n",
    "        3) d : it briefs whether we lost the game, True if it falls and False if it is still up.\n",
    "        \n",
    "        We keep the cartpole from falling by applying the continuous force according to the situation demands\n",
    "        \"\"\"\n",
    "        print(\"Parameter Observed:\",obs,r,d)\n",
    "        if d == True:\n",
    "            print(\"count:\",count)\n",
    "            count += 1\n",
    "            break\n",
    "env_temp.close()\n",
    "\n",
    "##################################################################################################################\n",
    "\n",
    "print(\"\\nWhen step(0) is supplied for action, the resultant change in parameters.\\n\")\n",
    "count = 0\n",
    "obs = env_temp.reset()\n",
    "while count <= 9:\n",
    "    for i in range(200):\n",
    "        time.sleep(1)\n",
    "        env_temp.render()\n",
    "        obs, r, d, _ = env_temp.step(1)\n",
    "        print(\"Parameter Observed:\",obs,r,d)\n",
    "        \"\"\"\n",
    "        Here the Parameters:\n",
    "        1) obs : These 4 values represent the cart position, cart velocity, pole angle, \n",
    "            and the velocity of the tip of the pole.\n",
    "        2) r : state, it's a positive number if it is above and negative if it falls.\n",
    "        3) d : it briefs whether we lost the game, True if it falls and False if it is still up\n",
    "        \n",
    "        We keep the cartpole from falling by applying the continuous force according to the situation demands\n",
    "        \"\"\"\n",
    "        if d == True:\n",
    "            print(\"count:\",count)\n",
    "            count += 1\n",
    "            break\n",
    "env_temp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/apoorve/.local/lib/python3.6/site-packages/tflearn/initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The Main task is to develop the agent/model which will take decision on it's own to deploy\n",
    "actions in the environment\n",
    "\n",
    "To that we will using Neural Network, with 3 dense layer (i.e. 3 inbetween layers) with \n",
    "Rectified linear unit function for each of them, and finally we have a output layer which\n",
    "give us probability for each of the possible action to be taken.\n",
    "\n",
    "\"\"\"\n",
    "observation = tflearn.input_data(shape=[None, 4]) # for storing the observation parameters \n",
    "net = tflearn.fully_connected(observation, 256, activation=\"relu\") # Dense Layer 1\n",
    "net = tflearn.fully_connected(net, 256, activation=\"relu\") # Dense Layer 2\n",
    "net = tflearn.fully_connected(net, 256, activation=\"relu\") # Dense Layer 3\n",
    "out = tflearn.fully_connected(net, 2, activation=\"softmax\") # final Output Layer\n",
    "\n",
    "reward_holder = tf.placeholder(tf.float32, [None]) # Variable storing reward i.e. state whether it falls or not\n",
    "action_holder = tf.placeholder(tf.int32, [None]) # Variable storing associated action for the respective state\n",
    "\n",
    "responsible_outputs = tf.gather(tf.reshape(out, [-1]), tf.range(0, tf.shape(out)[0] * tf.shape(out)[1], 2) + action_holder)\n",
    "# above is used to change the shape of accumulated outputs from different frames into single one\n",
    "\n",
    "\n",
    "loss = -tf.reduce_mean(tf.log(responsible_outputs) * reward_holder) # to specify the loss for the neural network\n",
    "optimizer = tf.train.AdamOptimizer() # function to be used to minimize the loss function across neural network\n",
    "update = optimizer.minimize(loss) # function to be used to update the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.455 4.465 3.44  6.   ]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.99 \n",
    "\"\"\"\n",
    "It is constant which we use to provide the amount of importance to the rewards which\n",
    "we get for each supplied actions\n",
    "\n",
    "The more it is nearer to 1 (such as 0.90-0.99), more weightage is assigned to the reward\n",
    "while more nearer to 0 (such as 0.01-0.10), very less weightage is assigned to the reward\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def discount_reward(rewards):\n",
    "    running_reward = 0\n",
    "    result = np.zeros_like(rewards)\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        result[i] = rewards[i] + gamma * running_reward\n",
    "        running_reward += rewards[i]\n",
    "    return result\n",
    "\n",
    "# Example\n",
    "print(discount_reward([5.0, 1.0, -2.5, 6.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apoorve/.local/lib/python3.6/site-packages/ipykernel_launcher.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For after 100 th episode, following weighted reward:  21.59\n",
      "For after 200 th episode, following weighted reward:  26.22\n",
      "For after 300 th episode, following weighted reward:  41.74\n",
      "For after 400 th episode, following weighted reward:  82.47\n",
      "For after 500 th episode, following weighted reward:  180.91\n",
      "For after 600 th episode, following weighted reward:  200.0\n",
      "WARNING:tensorflow:Issue encountered when serializing data_preprocessing.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing data_augmentation.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'NoneType' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1500 # No. of sessions, in which for each session we will try to acheive max possible reward\n",
    "max_time = 200 # Max times of frames the cart-pole is kept from falling, to result in victory in game environment\n",
    "all_rewards = [] #to keep track of rewards\n",
    "saver = tf.train.Saver() # To save our Trained model\n",
    "train_data = [] # To retrain the saved data\n",
    "Whole_sess_hist = [] # Here we will be storing whole history of trained parameters\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0 # Counter for each episode's final obtained reward\n",
    "        ep_history = [] # For storing each individual episode history\n",
    "        for j in range(max_time):\n",
    "            \"\"\"\n",
    "            Choose an random action and then evaluating the output probabilities, reason why we are picking\n",
    "            random action is due to the fact that we don't our agent to be biased towards single action but\n",
    "            should also know the consequences of the both the action for random situation\n",
    "            \"\"\"\n",
    "            a_one_hot = sess.run(out, feed_dict={observation: [obs]}).reshape(2) \n",
    "            action = np.random.choice(a_one_hot, p=a_one_hot) # Picking random action\n",
    "            action = np.argmax(a_one_hot == action)\n",
    "            obs1, r, d, _ = env.step(action) # Getting changed parameter for resultant random action\n",
    "            ep_history.append([obs, r, action]) # Storing those parameter for running episode\n",
    "#             Whole_sess_hist.append([obs,r,action])\n",
    "            obs = obs1 # Overwriting initial observation with new one\n",
    "            episode_reward += r\n",
    "            if d == True:\n",
    "                all_rewards.append(episode_reward)\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 1] = discount_reward(ep_history[:, 1])\n",
    "                train_data.extend(ep_history)\n",
    "                if i % 10 == 0 and i != 0:\n",
    "                    \"\"\"\n",
    "                    For every 10th episode, we will training the model again, by evaluating on saved\n",
    "                    previous trained data\n",
    "                    \"\"\"\n",
    "                    train_data = np.array(train_data)\n",
    "                    sess.run(update, feed_dict={observation: np.vstack(train_data[:, 0]),\n",
    "                                                    reward_holder: train_data[:, 1],\n",
    "                                                    action_holder: train_data[:, 2]})\n",
    "                    \"\"\"\n",
    "                    Above part is to update the weights associated with each episode to\n",
    "                    total data\n",
    "                    \"\"\"\n",
    "                    train_data = []\n",
    "                break\n",
    "                \n",
    "        if i % 100 == 0 and i != 0:\n",
    "            \"\"\"\n",
    "            For every 100th session of updating, we will be printing the average weightage reward\n",
    "            \"\"\"\n",
    "            print(\"For after\",str(i),\"th episode, following weighted reward: \",np.mean(all_rewards[-100:]))\n",
    "            Whole_sess_hist.append([obs[0],obs[0],obs[0],obs[0],r,action])\n",
    "            if np.mean(all_rewards[-100:]) == 200:\n",
    "                break\n",
    "            \n",
    "    saver.save(sess, \"/tmp/model.ckpt\") #For Saving the weights of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisation of the Average Reward during after training in each episode\n",
    "\n",
    "avg_reward = [np.mean(all_rewards[i-10:i+10]) for i in range(10, len(all_rewards))]\n",
    "# print(avg_reward)\n",
    "sns.lineplot(data=avg_reward[::10], markers=True, dashes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Whole_sess_hist)\n",
    "# sns.lineplot(data=Whole_sess_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model.ckpt\n",
      "Final Weighted Reward attained after 0th episode:  200.0\n",
      "Final Weighted Reward attained after 1th episode:  200.0\n",
      "Final Weighted Reward attained after 2th episode:  200.0\n",
      "Final Weighted Reward attained after 3th episode:  200.0\n",
      "Final Weighted Reward attained after 4th episode:  200.0\n",
      "Final Weighted Reward attained after 5th episode:  200.0\n",
      "Final Weighted Reward attained after 6th episode:  200.0\n",
      "Final Weighted Reward attained after 7th episode:  200.0\n",
      "Final Weighted Reward attained after 8th episode:  200.0\n",
      "Final Weighted Reward attained after 9th episode:  200.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here now, since our model is trained, we will use the trained weights\n",
    "for testing and then will output the final reward for each episode which \n",
    "is in total 200 frames long.\n",
    "\"\"\"\n",
    "\n",
    "max_time = 200 # For how much frame the cart-pole being need to kept stable to have Victory\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, \"/tmp/model.ckpt\") # Restoring the Trained Model Weights\n",
    "    #Show the results\n",
    "    for i in range(10):\n",
    "        \n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        for j in range(max_time):\n",
    "            \n",
    "            #Choose an random action for testing\n",
    "            a_one_hot = sess.run(out, feed_dict={observation: [obs]}).reshape(2)\n",
    "            action = np.random.choice(a_one_hot, p=a_one_hot)\n",
    "            action = np.argmax(a_one_hot == action)\n",
    "            env.render()\n",
    "            time.sleep(0.005)\n",
    "            obs, r, d, _ = env.step(action)\n",
    "            episode_reward += r\n",
    "            if d == True:\n",
    "                break\n",
    "        print(\"Final Weighted Reward attained after \"+str(i)+\"th episode: \",episode_reward) \n",
    "        \"\"\"\n",
    "        Giving final reward for each output, after getting actions\n",
    "        \"\"\"\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}